{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import json\n",
      "import nltk\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dfTrain = pd.read_csv('../data/train.tsv',sep = '\\t')\n",
      "dfTest = pd.read_csv('../data/test.tsv', sep = '\\t')\n",
      "\n",
      "df = dfTrain.append(dfTest,ignore_index=True)\n",
      "\n",
      "df['setType'] = 'training'\n",
      "df['setType'][np.isnan(df.label)] = 'test'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Extract features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getBoilerPlateText(text, obj = 'title'):    \n",
      "    try:\n",
      "        result = json.loads(text)[obj].encode('utf-8').strip()    \n",
      "    except:  # if there is no title, put in blank\n",
      "        result = ''\n",
      "    return result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "boilerPlateTitles = df.boilerplate.apply(getBoilerPlateText,obj = 'title')\n",
      "boilerPlateBodies = df.boilerplate.apply(getBoilerPlateText,obj =  'body')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
      "\n",
      "# Vectorizer will by default remove punctuation and convert to lower case. We just need to strip the stop words before training our classifier.\n",
      "vectorizer = TfidfVectorizer(min_df=2,stop_words='english',binary=False, analyzer='word', use_idf = True, smooth_idf = True, sublinear_tf = True) # use binary option to check for instances instead of summing instances\n",
      "bodyVect = vectorizer.fit_transform(boilerPlateBodies)\n",
      "titleVect = vectorizer.fit_transform(boilerPlateTitles)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def countText(text, pattern, relative = True):\n",
      "    import re\n",
      "    result = len(re.findall(pattern, text))\n",
      "    if relative == True:\n",
      "        result = result / np.double(countWords(text))\n",
      "    if text == '':\n",
      "        result = 0\n",
      "    return result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def countWords(text):\n",
      "    import nltk\n",
      "    if text == '':\n",
      "        result = 0\n",
      "    else: \n",
      "        result = len(nltk.word_tokenize(text))\n",
      "    return result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "keyTerms = ['\\,',\n",
      "            '\\!',\n",
      "            'butter',\n",
      "            'water',\n",
      "            'cup',\n",
      "            'degree',\n",
      "            'recipe'\n",
      "            'comment',\n",
      "            'email',\n",
      "            'contact',\n",
      "            '[0-9]{4}',\n",
      "            '\\|',\n",
      "            '\\d',\n",
      "            '@',\n",
      "            'calories',\n",
      "            'sign?in',\n",
      "            'log?in',\n",
      "            'register'\n",
      "            ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "textFeatures = pd.DataFrame(index = df.index)\n",
      "for term in keyTerms:\n",
      "    textFeatures['bpBodyContains_' + term] = boilerPlateBodies.apply(countText, pattern = term, relative = True)\n",
      "    textFeatures['bpTitleContains_' + term] = boilerPlateTitles.apply(countText, pattern = term, relative = True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getUrlDomain(url):\n",
      "    import urlparse\n",
      "    import re\n",
      "    parsed = urlparse.urlparse(url)\n",
      "    domain = re.sub('^www\\.', '', parsed.netloc).split('.')\n",
      "    result = domain[len(domain)-1]\n",
      "    return result\n",
      "\n",
      "def getName(url):\n",
      "    import urlparse\n",
      "    import re\n",
      "    parsed = urlparse.urlparse(url)\n",
      "    domain = re.sub('^www\\.', '', parsed.netloc).split('.')\n",
      "    result = domain[0]\n",
      "    return result\n",
      "\n",
      "def countPaths(url):\n",
      "    import urlparse\n",
      "    import re\n",
      "    parsed = urlparse.urlparse(url)\n",
      "    domain = re.findall('\\/',parsed.path)\n",
      "    return len(domain)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "domains = df.url.apply(getUrlDomain)\n",
      "webNames = df.url.apply(getName)\n",
      "numPaths = df.url.apply(countPaths)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "domains_df = pd.get_dummies(domains, prefix = 'DUM')\n",
      "webNames_df = pd.get_dummies(webNames, prefix = 'DUM')\n",
      "alchemyCategories_df = pd.get_dummies(df.alchemy_category, prefix = 'DUM')\n",
      "numPaths_df = pd.get_dummies(numPaths, prefix = 'DUM')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "originalFeatureList =['avglinksize','commonlinkratio_1','commonlinkratio_2','commonlinkratio_3','commonlinkratio_4', \n",
      "           'compression_ratio','embed_ratio','frameTagRatio', 'framebased', 'hasDomainLink', 'html_ratio', 'image_ratio', \n",
      "           'is_news', 'lengthyLinkDomain', 'linkwordscore', 'news_front_page', 'non_markup_alphanum_characters', 'numberOfLinks',\n",
      "           'numwords_in_url', 'parametrizedLinkRatio', 'spelling_errors_ratio']\n",
      "\n",
      "originalFeatures = df[originalFeatureList]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import ElasticNetCV, ElasticNet\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import TruncatedSVD\n",
      "\n",
      "titleDecomposed = TruncatedSVD(random_state = 123, n_components = 10).fit_transform(titleVect)\n",
      "bodyDecomposed = TruncatedSVD(random_state = 123, n_components = 50).fit_transform(bodyVect)\n",
      "\n",
      "textDecomposed = np.hstack([titleDecomposed,bodyDecomposed])\n",
      "textDecomposed_training = textDecomposed[df.setType == 'training']\n",
      "textDecomposed_test = textDecomposed[df.setType == 'test']\n",
      "\n",
      "\n",
      "# used for estimating alch cats\n",
      "titleDecomposed2 = TruncatedSVD(random_state = 123, n_components = 100).fit_transform(titleVect)\n",
      "bodyDecomposed2 = TruncatedSVD(random_state = 123, n_components = 500).fit_transform(bodyVect)\n",
      "textDecomposed2 = np.hstack([titleDecomposed2,bodyDecomposed2])\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Construct Training and Test Data\n",
      "originalFeatures_training = originalFeatures[df.setType == 'training']\n",
      "textFeatures_training = textFeatures[df.setType == 'training']\n",
      "domains_training = domains_df[df.setType == 'training']\n",
      "webNames_training = webNames_df[df.setType == 'training']\n",
      "numPaths_training = numPaths_df[df.setType == 'training']\n",
      "alchemyCategories_training = alchemyCategories_df[df.setType == 'training']\n",
      "\n",
      "Y = df[df.setType == 'training'].label\n",
      "X = np.concatenate((originalFeatures_training.values, textFeatures_training, numPaths_training,\n",
      "                    domains_training.values, webNames_training.values,alchemyCategories_training.values), 1)\n",
      "X[X == '?'] = 0\n",
      "\n",
      "\n",
      "# Construct ALTERNATE Training and Test Data\n",
      "X_alt = np.hstack([X, textDecomposed_training])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "originalFeatures_test = originalFeatures[df.setType == 'test']\n",
      "textFeatures_test = textFeatures[df.setType == 'test']\n",
      "domains_test = domains_df[df.setType == 'test']\n",
      "webNames_test = webNames_df[df.setType == 'test']\n",
      "numPaths_test = numPaths_df[df.setType == 'test']\n",
      "alchemyCategories_test = alchemyCategories_df[df.setType == 'test']\n",
      "\n",
      "X_Test = np.concatenate((originalFeatures_test.values, textFeatures_test, numPaths_test,\n",
      "                    domains_test.values, webNames_test.values,alchemyCategories_test.values), 1)\n",
      "X_Test[X_Test == '?'] = 0\n",
      "\n",
      "X_Test_alt = np.hstack([X_Test, textDecomposed_test])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "submission = pd.DataFrame(df[df.setType == 'test'].urlid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def crossVal(clf, X_data, Y_data, NFOLDS = 10): \n",
      "    import sklearn.ensemble as ens\n",
      "    from sklearn import metrics\n",
      "    from sklearn.cross_validation import StratifiedKFold\n",
      "\n",
      "    skf = StratifiedKFold(Y_data, n_folds = NFOLDS)\n",
      "    auc = []\n",
      "    for train_index, test_index in skf:\n",
      "        x_train, x_test = X_data[train_index], X_data[test_index]\n",
      "        y_train, y_test = Y_data[train_index], Y_data[test_index]\n",
      "        \n",
      "        clf.fit(x_train, y_train)\n",
      "        \n",
      "        try: \n",
      "            probs = clf.predict_proba(x_test)\n",
      "            y_hat = probs[:,1]\n",
      "        except:\n",
      "            y_hat = clf.predict(x_test)\n",
      "        auc.append(metrics.roc_auc_score(y_test, y_hat))\n",
      "    print auc\n",
      "    print np.mean(auc)\n",
      "    return auc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 164
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "elasticCV = ElasticNetCV(l1_ratio= [.1, .5, .7, .9, .95, .99, 1], n_jobs = 2)\n",
      "elasticCV.fit(X.astype(np.float),Y)\n",
      "alpha_star = elasticCV.alpha_\n",
      "l1_ratio_star = elasticCV.l1_ratio_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 237,
       "text": [
        "ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
        "       l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1], max_iter=1000,\n",
        "       n_alphas=100, n_jobs=1, normalize=False, precompute='auto',\n",
        "       tol=0.0001, verbose=0)"
       ]
      }
     ],
     "prompt_number": 237
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# l1: 0.8, alpha = 1\n",
      "\n",
      "clf_EN_1 = ElasticNet(alpha = alpha_star, l1_ratio=l1_ratio_star)\n",
      "results = crossVal(clf_EN_1,X.astype(np.float),Y,10)\n",
      "print 'alpha: ' + str(alpha_star)\n",
      "print 'rho: ' + str(l1_ratio_star)\n",
      "print results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "alpha: 0.589482653704\n",
        "rho: 0.1\n",
        "[0.62111842105263071, 0.6539254385964911, 0.62657163742690059, 0.64047514619882906, 0.59868421052631571, 0.62637056581647621, 0.62775579009088234, 0.65041776605101154, 0.57822486074464952, 0.6257147045887701]\n"
       ]
      }
     ],
     "prompt_number": 239
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rf = RandomForestClassifier(criterion = 'gini',random_state = 123,\n",
      "                                 min_samples_leaf = 2, min_samples_split = 6, n_estimators = 300)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr =  LogisticRegression(random_state = 123, penalty = 'l1', C = 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rf.fit(X_alt, Y)\n",
      "lr.fit(X_alt, Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'alt' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-22-a5e2a2773c97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_alt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprobs_rf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_Test_alt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprobs_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_Test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mfinalPrediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprobs_rf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs_lr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0msubmission\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinalPrediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'alt' is not defined"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "probs_rf = rf.predict_proba(X_Test_alt)[:,1]\n",
      "probs_lr = lr.predict_proba(X_Test_alt.astype(np.float))[:,1]\n",
      "finalPrediction = np.vstack([probs_rf, probs_lr]).mean(0)\n",
      "submission['label'] = finalPrediction.T\n",
      "submission.to_csv('submission_21.csv', index = False, sep = ',')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Train chosen classifier and predict on test set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "finalPrediction = np.vstack([y_hat_GBM, y_hat_RF]).mean(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 159
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print RF_Best_alt_cv\n",
      "print np.mean(RF_Best_alt_cv)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.86701754385964902, 0.85896929824561297, 0.88310672514619903, 0.87345760233918102, 0.86160818713450316, 0.89448841981823568, 0.86003371445323962, 0.87238346525945376, 0.8564057461155089, 0.85340126081219814]\n",
        "0.868087196318\n"
       ]
      }
     ],
     "prompt_number": 159
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "parameters3 = {'learning_rate': [0.05, 0.1, 0.25, 0.5, 0.75],\n",
      "              'n_estimators' : [100, 300, 500, 700, 1000, 2000, 4000],\n",
      "              'max_depth' : [2,3,4,5,10,15,20,50,100],\n",
      "              'min_samples_split' : [2,4,6,8,10],\n",
      "              'min_samples_leaf' : [1,2,5],\n",
      "              'subsample' : [0.5, 0.75, 1]}\n",
      "clfGBMGrid = GridSearchCV(GradientBoostingClassifier(random_state = 123), parameters3).fit(X,Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr = LogisticRegression(random_state = 123, penalty = 'l1', C = 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = crossVal(lr, X_alt.astype(np.float), Y, 10)\n",
      "print results\n",
      "print np.mean(results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.86211257309941691, 0.8669809941520461, 0.88327485380117032, 0.88035087719298522, 0.85097953216374278, 0.88353855174435636, 0.86282615068894819, 0.877814423922603, 0.84919378481383645, 0.85499193666617901]\n",
        "0.867206367825\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import AdaBoostClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ada_rf = AdaBoostClassifier(base_estimator=rf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = crossVal(ada_rf, X_alt, Y)\n",
      "print results\n",
      "print np.mean(results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.87054824561403465, 0.8634429824561427, 0.88392543859649209, 0.87408625730994072, 0.86663742690058687, 0.89635004397537366, 0.86713573732043414, 0.87586484901788386, 0.85677953679273, 0.85437619117431451]\n",
        "0.870914670916\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
      "from sklearn.metrics import confusion_matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alch_cat_training = df[(df.alchemy_category != '?') & (df.alchemy_category != 'unknown') & (df.setType == 'training')].alchemy_category"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predictors = textDecomposed2[(df.alchemy_category != '?') & (df.alchemy_category != 'unknown') & (df.setType == 'training')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alch_cats = alch_cat_training.unique()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "i = 0\n",
      "for item in alch_cats:\n",
      "    alch_cat_training[alch_cat_training == item] = i\n",
      "    i += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf2 = BernoulliNB(alpha = 0.5, fit_prior=True)\n",
      "clf2.fit(predictors, alch_cat_training.astype(np.int))\n",
      "predicted = clf2.predict(predictors)\n",
      "result = pd.DataFrame(confusion_matrix(alch_cat_training.astype(np.int), predicted), columns = alch_cats, index = alch_cats)\n",
      "print result\n",
      "estAlchCat = clf2.predict(textDecomposed2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                    business  recreation  health  sports  arts_entertainment  \\\n",
        "business                 482         159      37      10                  83   \n",
        "recreation               209         680      37      36                 118   \n",
        "health                    26          18     385       6                  19   \n",
        "sports                    12          15      13     280                  28   \n",
        "arts_entertainment       141         121      27      62                 473   \n",
        "science_technology        47          27      28       3                  24   \n",
        "gaming                     8          10       6      12                  12   \n",
        "culture_politics          36          60      17       8                  43   \n",
        "computer_internet         14          39       6       3                  28   \n",
        "law_crime                  5           3       0       0                   7   \n",
        "religion                   9           8       8       3                  16   \n",
        "weather                    0           0       0       0                   1   \n",
        "\n",
        "                    science_technology  gaming  culture_politics  \\\n",
        "business                            30       1                15   \n",
        "recreation                          27       1                32   \n",
        "health                              14       0                12   \n",
        "sports                               5       0                10   \n",
        "arts_entertainment                  14       2                33   \n",
        "science_technology                 126       0                 9   \n",
        "gaming                               0      13                 1   \n",
        "culture_politics                    10       0               146   \n",
        "computer_internet                    9       0                 4   \n",
        "law_crime                            0       0                 2   \n",
        "religion                             3       0                 5   \n",
        "weather                              0       0                 0   \n",
        "\n",
        "                    computer_internet  law_crime  religion  weather  \n",
        "business                           29          0         1       33  \n",
        "recreation                         28          0         1       60  \n",
        "health                              5          0         0       21  \n",
        "sports                              2          0         1       14  \n",
        "arts_entertainment                 23          0         2       43  \n",
        "science_technology                 11          0         0       14  \n",
        "gaming                              5          0         0        9  \n",
        "culture_politics                    7          1         0       15  \n",
        "computer_internet                 178          0         1       14  \n",
        "law_crime                           0         11         0        3  \n",
        "religion                            1          0        12        7  \n",
        "weather                             0          0         0        3  \n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "estAlchCat[(df.alchemy_category != '?') & (df.alchemy_category != 'unknown') & (df.setType == 'training')] = alch_cat_training[(df.alchemy_category != '?') & (df.alchemy_category != 'unknown') & (df.setType == 'training')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alchemyCategories2_df = pd.get_dummies(estAlchCat, prefix = 'DUM')\n",
      "alchemyCategories2_training = alchemyCategories2_df[df.setType == 'training']\n",
      "alchemyCategories2_test = alchemyCategories2_df[df.setType == 'test']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_alt_2 = X = np.hstack([originalFeatures_training.values, textFeatures_training, numPaths_training,\n",
      "                    domains_training.values, webNames_training.values,alchemyCategories2_training.values, textDecomposed_training])\n",
      "\n",
      "X_Test_2 = np.hstack([originalFeatures_test.values, textFeatures_test, numPaths_test,\n",
      "                    domains_test.values, webNames_test.values,alchemyCategories2_test.values, textDecomposed_test])\n",
      "\n",
      "X_alt_2[X_alt_2 == '?'] = 0\n",
      "X_Test_2[X_Test_2 == '?'] = 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = crossVal(rf, X_alt_2, Y)\n",
      "print results\n",
      "print np.mean(results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.86582602339181425, 0.86538011695906492, 0.88462719298245562, 0.87537280701754439, 0.86476608187134452, 0.89866608032834916, 0.86748021108179463, 0.87712547639988148, 0.85537965406039185, 0.85117284855592923]\n",
        "0.870579649265\n"
       ]
      }
     ],
     "prompt_number": 154
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = crossVal(lr, X_alt_2.astype(np.float), Y)\n",
      "print results\n",
      "print np.mean(results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.86722953216374221, 0.86760964912280591, 0.88595760233918197, 0.87606725146198894, 0.85593567251462133, 0.88792143066549356, 0.86999413661682912, 0.87944151275285798, 0.85029316915860287, 0.85405365782143294]\n",
        "0.869450361462\n"
       ]
      }
     ],
     "prompt_number": 153
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import GradientBoostingClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gbm = GradientBoostingClassifier(random_state=123)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = crossVal(gbm, X_alt_2, Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.86604532163742776, 0.87507309941520584, 0.88288377192982503, 0.8753106725146188, 0.87012792397660765, 0.89775359132219124, 0.87153693931398435, 0.87915200820873529, 0.85851289944297993, 0.85981527635244115]\n",
        "0.873621150411\n"
       ]
      }
     ],
     "prompt_number": 167
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "crossVal(lr, X_alt_2.astype(np.float), Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.86722953216374221, 0.86760964912280591, 0.88595760233918197, 0.87606725146198894, 0.85593567251462133, 0.88792143066549356, 0.86999413661682912, 0.87944151275285798, 0.85029316915860287, 0.85405365782143294]\n",
        "0.869450361462\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 177,
       "text": [
        "[0.86722953216374221,\n",
        " 0.86760964912280591,\n",
        " 0.88595760233918197,\n",
        " 0.87606725146198894,\n",
        " 0.85593567251462133,\n",
        " 0.88792143066549356,\n",
        " 0.86999413661682912,\n",
        " 0.87944151275285798,\n",
        " 0.85029316915860287,\n",
        " 0.85405365782143294]"
       ]
      }
     ],
     "prompt_number": 177
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gbm_parameters = {'n_estimators':[20, 50,100,200,500,1000]}\n",
      "gbm = GradientBoostingClassifier(random_state = 123, max_depth = 6)\n",
      "gbm_grid = GridSearchCV(gbm, gbm_parameters, verbose = 1, n_jobs = 4).fit(X_alt_2,Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gbm_grid.best_estimator_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}